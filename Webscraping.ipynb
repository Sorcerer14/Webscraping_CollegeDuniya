{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Webscraping.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4OjvB396SYPq9JPaBvn6w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sorcerer14/Webscraping_CollegeDuniya/blob/main/Webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCpaHgk6g9Tk",
        "outputId": "1d0f13cf-688e-469c-9f8c-92c5de136e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: use options instead of chrome_options\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n"
          ]
        }
      ],
      "source": [
        "urls = []\n",
        "urls.append(\"https://collegedunia.com/moradabad-colleges\") #Creating URL to directed site\n",
        "\n",
        "no_of_pagedowns = 1\n",
        "\n",
        "#Extracting Data from Website\n",
        "#BeautifulSoup to extract contents from the website.\n",
        "#requests\n",
        "\n",
        "#Scrolling through the page dynamically\n",
        "#selenium for using the chrome webdriver\n",
        "#time to wait for a while and scroll down the page\n",
        "#Also Needs Chrome Driver to be explicitely installed to work with dynamicaly scrolling down pages ###\n",
        "\n",
        "#Pandas to export the stored dataframe as a csv\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import requests\n",
        "import time\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import pandas as pd\n",
        "\n",
        "for u in urls:  #iterates throught the list urls\n",
        "    save_name = u[25:]+'.csv'       # specifies the name pandas has to store it as csv or xlxs\n",
        "    save_name = save_name.replace(\"/\",\"-\")      # replaces / with - as the file name in the case would be invalid\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    browser = webdriver.Chrome(chrome_options=options)  # Starting browser with specified options\n",
        "    browser.get(u)                              # opens the browser with the url from the iterator\n",
        "    time.sleep(1)                               # waits for the page to load\n",
        "    elem = browser.find_element_by_tag_name(\"body\")     # gets the body tag from the html under which content is present\n",
        "    while no_of_pagedowns:                      # scrolls the pages to the numeber of times\n",
        "        elem.send_keys(Keys.PAGE_DOWN)          # pressed the down key\n",
        "        time.sleep(0.4)                         # waits for the page to load\n",
        "        no_of_pagedowns-=1                      # reduce the number of pagedowns by 1\n",
        "    #Initiate a Pandas DataFrame with the Specific Columns\n",
        "    df = pd.DataFrame(columns=['S No','College Name', 'Details','Course','Fees','Eligibility-Criteria','weblink'])\n",
        "    html = browser.page_source              # gets the page source once it's loaded\n",
        "    main_page_content = BeautifulSoup(html)     # extracts the features using bs4\n",
        "    browser.close()                         # closes the broswer ( important !!)\n",
        "    Content = []                            #Initialize an empty list\n",
        "    # Iterate throught the no of list of colleges\n",
        "    for i in range(len(main_page_content.find_all(\"div\", {\"class\": \"jsx-765939686 text-white\"}))):\n",
        "        paragraphs = main_page_content.find_all(\"div\", {\"class\": \"cdcms_college_highlights\"})[i]\n",
        "        link=url= paragraphs.a['href']              #gets the url of the colleges *NOT USED IN TASK BUT HELPFULL*\n",
        "        name = paragraphs.text                      #gets the name of the college\n",
        "        Content.append(paragraphs.text)             # Appends the name to the content list\n",
        "        link = url                                \n",
        "        response = requests.get(link, timeout=10)   # get to fetch the details inside the url\n",
        "        page_content = BeautifulSoup(response.content, \"html.parser\")  # parses the data using bs4\n",
        "        textContent = []                            #empty list and it has lots of important data\n",
        "        # this has the important data we now extract from\n",
        "        for k in range(len(page_content.find_all(\"div\",{\"class\": \"jsx-765939686 text-white\"}))):\n",
        "            paragraphs_new = page_content.find_all(\"div\",{\"class\": \"cdcms_college_highlights\"})[k].text\n",
        "            textContent.append(paragraphs_new)\n",
        "        \n",
        "        #This is most important part of this project where we join/clean all the data that we got from various sources\n",
        "        #The insights are :-\n",
        "        # 1) All colleges are from India and a particular area\n",
        "        # 2) Content used is from top-most present user\n",
        "        # 3) The table used first splitted into three parts i.e. course,fee-structure and eligibility-criteria\n",
        "        # NOTE : There were lots of garbage data that was cleaned in below process\n",
        "\n",
        "        for l in range(len(textContent)):\n",
        "            start = textContent[l].find('India')\n",
        "            start_weblink = textContent[l].find('http')\n",
        "            http_val = textContent[l].find('http')\n",
        "            if(start>=0):\n",
        "                details = textContent[l][0:start+5]\n",
        "                course = textContent[l][4:start+0]\n",
        "                fees = textContent[l][4:start+1]\n",
        "                eligibility_criteria = textContent[l][4:start+2]\n",
        "                weblink = textContent[l][start_weblink:]\n",
        "                while(details.find(\"\\n\")>0):\n",
        "                    loc = details.find(\"\\n\")\n",
        "                    details = details.replace('\\n',',')\n",
        "        deatails=details.replace(',    ',',')\n",
        "\n",
        "        textContent = []\n",
        "        for o in range(len(page_content.find_all(\"script\",{\"type\": \"application/ld+json\"}))):\n",
        "            paragraphs = page_content.find_all(\"script\",{\"type\": \"application/ld+json\"})[o].text\n",
        "            textContent.append(paragraphs)\n",
        "        \n",
        "        #Prints the name of the colleges\n",
        "        #Create a list to store the detais and finally dumps into there Pandas dataframe\n",
        "        print(name)\n",
        "        list=[]\n",
        "        list.append(i+1)\n",
        "        list.append(name)\n",
        "        list.append(details)\n",
        "        list.append(course)\n",
        "        list.append(fees)\n",
        "        list.append(eligibility_criteria)\n",
        "        list.append(weblink)\n",
        "        #it now stores data in dataframe\n",
        "        df.loc[i+1]=list\n",
        "        #pandas stores it to the specified csv file from here.\n",
        "        df.to_csv(r'/content/web_data/'+save_name,index=None,encoding='utf-8')"
      ]
    }
  ]
}